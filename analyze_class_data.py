"""
ä½“æµ‹æ•°æ®åˆ†ææ¨¡å—ï¼šè§£æç­çº§ä½“æµ‹æ•°æ®ï¼Œç”Ÿæˆå’Œæ›´æ–°class_profiles.json
æä¾›APIæ¥å£ä¾›Flaskåº”ç”¨è°ƒç”¨
ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ†æ
"""
import pandas as pd
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Generator
import io

# å¹´çº§ç¼–å·åˆ°å¹´çº§åç§°çš„æ˜ å°„
GRADE_MAPPING = {
    14: "1", 15: "2", 16: "3", 17: "4", 18: "5",
    19: "6", 20: "7", 21: "8", 22: "9"
}

# æ•°æ®åº“è§„å®šçš„6ä¸ªè–„å¼±ç»´åº¦
ALLOWED_WEAKNESSES = ["å½¢æ€", "è€åŠ›", "åŠ›é‡", "æŸ”éŸ§", "é€Ÿåº¦", "æœºèƒ½"]

# ä½“æµ‹é¡¹ç›®åˆ°è–„å¼±ç»´åº¦çš„æ˜ å°„ï¼ˆåªèƒ½ä½¿ç”¨æ•°æ®åº“è§„å®šçš„6ä¸ªç»´åº¦ï¼‰
WEAKNESS_MAPPING = {
    "50ç±³è·‘": "é€Ÿåº¦",
    "ä¸€åˆ†é’Ÿä»°å§èµ·å": "åŠ›é‡",
    "å¼•ä½“å‘ä¸Š": "åŠ›é‡",
    "åä½ä½“å‰å±ˆ": "æŸ”éŸ§",
    "ä¸€åˆ†é’Ÿè·³ç»³": "é€Ÿåº¦",  # è·³ç»³å½’å…¥é€Ÿåº¦
    "ç«‹å®šè·³è¿œ": "åŠ›é‡",    # çˆ†å‘åŠ›å½’å…¥åŠ›é‡
    "800ç±³è·‘": "è€åŠ›",
    "1000ç±³è·‘": "è€åŠ›",
    "è‚ºæ´»é‡": "æœºèƒ½",      # å¿ƒè‚ºåŠŸèƒ½å½’å…¥æœºèƒ½
    "èº«é«˜": "å½¢æ€",
    "ä½“é‡": "å½¢æ€",
    "BMI": "å½¢æ€"
}

def analyze_class_weakness(df: pd.DataFrame, class_name: str) -> Tuple[List[str], Dict[str, str], Dict[str, str]]:
    """
    åˆ†æç­çº§çš„è–„å¼±é¡¹

    è¿”å›: (è–„å¼±é¡¹åˆ—è¡¨, è–„å¼±é¡¹è¯¦ç»†ä¿¡æ¯å­—å…¸, è–„å¼±é¡¹å¯¹åº”çš„ä½“æµ‹é¡¹ç›®å­—å…¸)
    """
    weaknesses = []
    weakness_details = {}
    weakness_items = {}  # è®°å½•æ¯ä¸ªè–„å¼±ç»´åº¦å¯¹åº”çš„ä½“æµ‹é¡¹ç›®

    # åˆ†æå„é¡¹ä½“æµ‹æ•°æ®çš„ç­‰çº§åˆ†å¸ƒï¼ˆåªä½¿ç”¨æ•°æ®åº“è§„å®šçš„6ä¸ªç»´åº¦ï¼‰
    test_items = {
        "50ç±³è·‘": "é€Ÿåº¦",
        "ä¸€åˆ†é’Ÿä»°å§èµ·å": "åŠ›é‡",
        "åä½ä½“å‰å±ˆ": "æŸ”éŸ§",
        "ä¸€åˆ†é’Ÿè·³ç»³": "é€Ÿåº¦",
        "ç«‹å®šè·³è¿œ": "åŠ›é‡",
        "800ç±³è·‘": "è€åŠ›",
        "1000ç±³è·‘": "è€åŠ›",
        "è‚ºæ´»é‡": "æœºèƒ½",
        "èº«é«˜": "å½¢æ€",
        "ä½“é‡": "å½¢æ€"
    }

    weakness_scores = {}
    
    for item, dimension in test_items.items():
        grade_col = f"{item}ç­‰çº§"
        if grade_col not in df.columns:
            continue
            
        # ç»Ÿè®¡ç­‰çº§åˆ†å¸ƒ
        grade_counts = df[grade_col].value_counts()
        total = len(df[df[grade_col].notna()])
        
        if total == 0:
            continue
        
        # è®¡ç®—ä¼˜ç§€ç‡å’ŒåŠæ ¼ç‡
        excellent_count = grade_counts.get("ä¼˜ç§€", 0)
        good_count = grade_counts.get("è‰¯å¥½", 0)
        pass_count = grade_counts.get("åŠæ ¼", 0)
        fail_count = grade_counts.get("ä¸åŠæ ¼", 0)
        
        excellent_rate = excellent_count / total * 100
        good_rate = good_count / total * 100
        pass_rate = pass_count / total * 100
        fail_rate = fail_count / total * 100
        
        # è®¡ç®—è–„å¼±åˆ†æ•°ï¼ˆä¼˜ç§€ç‡è¶Šä½ã€åŠæ ¼ç‡è¶Šé«˜ï¼Œåˆ†æ•°è¶Šé«˜è¡¨ç¤ºè¶Šè–„å¼±ï¼‰
        weakness_score = (100 - excellent_rate) + pass_rate + fail_rate * 2
        weakness_scores[dimension] = {
            "score": weakness_score,
            "item": item,
            "excellent_rate": excellent_rate,
            "good_rate": good_rate,
            "pass_rate": pass_rate,
            "fail_rate": fail_rate,
            "excellent_count": excellent_count,
            "good_count": good_count,
            "pass_count": pass_count,
            "fail_count": fail_count,
            "total": total
        }
    
    # æ‰¾å‡ºæœ€è–„å¼±çš„2ä¸ªç»´åº¦
    sorted_weaknesses = sorted(weakness_scores.items(), key=lambda x: x[1]["score"], reverse=True)

    for dimension, stats in sorted_weaknesses[:2]:
        weaknesses.append(dimension)
        weakness_items[dimension] = stats['item']  # è®°å½•å¯¹åº”çš„ä½“æµ‹é¡¹ç›®

        # ç”Ÿæˆè¯¦ç»†æè¿°
        detail = f"ä»ä½“æµ‹æ•°æ®æ¥çœ‹ï¼Œ{dimension}æ˜¯{class_name}çš„è–„å¼±é¡¹ï¼š{stats['item']}"

        if stats['excellent_count'] == 0:
            detail += f"æ— 'ä¼˜ç§€'ç­‰çº§å­¦ç”Ÿï¼Œ"
        else:
            detail += f"ä»…{stats['excellent_count']}äººï¼ˆå æ¯”{stats['excellent_rate']:.1f}%ï¼‰è¾¾åˆ°'ä¼˜ç§€'ï¼Œ"

        if stats['good_count'] > 0:
            detail += f"{stats['good_count']}äººï¼ˆå æ¯”{stats['good_rate']:.1f}%ï¼‰è¾¾åˆ°'è‰¯å¥½'ï¼Œ"

        detail += f"{stats['pass_count']}äººï¼ˆå æ¯”{stats['pass_rate']:.1f}%ï¼‰ä¸º'åŠæ ¼'"

        if stats['fail_count'] > 0:
            detail += f"ï¼Œ{stats['fail_count']}äººï¼ˆå æ¯”{stats['fail_rate']:.1f}%ï¼‰ä¸º'ä¸åŠæ ¼'"

        detail += f"ï¼Œ{dimension}ç´ è´¨æå‡éœ€æ±‚è¿«åˆ‡ã€‚"

        weakness_details[dimension] = detail

    return weaknesses, weakness_details, weakness_items


def analyze_class_file(file_path: Path) -> Dict:
    """åˆ†æå•ä¸ªç­çº§æ–‡ä»¶"""
    df = pd.read_excel(file_path)
    class_name = file_path.stem  # ä¾‹å¦‚ï¼šä¸€å¹´çº§1ç­

    # è·å–å¹´çº§ç¼–å·
    grade_code = df['å¹´çº§ç¼–å·'].iloc[0] if len(df) > 0 else 14
    grade_query = GRADE_MAPPING.get(grade_code, "1")

    # åˆ†æè–„å¼±é¡¹
    weaknesses, weakness_details, weakness_test_items = analyze_class_weakness(df, class_name)

    # æ„å»ºç­çº§é…ç½®
    # ç”Ÿæˆç®€æ´çš„æè¿°
    weakness_desc_items = []
    for weakness in weaknesses:
        test_item = weakness_test_items.get(weakness, "")
        if test_item:
            weakness_desc_items.append(f"{weakness}ï¼ˆ{test_item}ï¼‰")
        else:
            weakness_desc_items.append(weakness)

    description = f"{class_name}ä½“è´¨ç›‘æµ‹æ ¸å¿ƒè–„å¼±ç»´åº¦ï¼š" + "ã€".join(weakness_desc_items) if weakness_desc_items else f"{class_name}ä½“è´¨ç›‘æµ‹æ•°æ®"

    profile = {
        "grades_query": grade_query,
        "trained_weaknesses": "ã€".join(weaknesses) if weaknesses else "",
        "count_query": "",
        "semantic_query": "",
        "description": description,
        "weakness_details": weakness_details
    }

    return class_name, profile


def generate_class_profiles(class_data_dir="class_data", output_file="prompts/class_profiles.json", max_classes=10):
    """
    ç”Ÿæˆclass_profiles.jsonæ–‡ä»¶
    
    å‚æ•°:
        class_data_dir: ç­çº§æ•°æ®æ–‡ä»¶å¤¹
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        max_classes: æœ€å¤šå¤„ç†å¤šå°‘ä¸ªç­çº§ï¼ˆç”¨äºæµ‹è¯•ï¼Œè®¾ä¸ºNoneå¤„ç†å…¨éƒ¨ï¼‰
    """
    class_data_path = Path(class_data_dir)
    profiles = {}
    
    # è·å–æ‰€æœ‰ç­çº§æ–‡ä»¶
    class_files = sorted(class_data_path.glob("*.xlsx"))
    
    if max_classes:
        class_files = class_files[:max_classes]
    
    print(f"å¼€å§‹åˆ†æ {len(class_files)} ä¸ªç­çº§...")
    
    for idx, file_path in enumerate(class_files, 1):
        try:
            class_name, profile = analyze_class_file(file_path)
            profiles[class_name] = profile
            print(f"[{idx}/{len(class_files)}] åˆ†æå®Œæˆ: {class_name}")
        except Exception as e:
            print(f"[{idx}/{len(class_files)}] åˆ†æå¤±è´¥: {file_path.name}, é”™è¯¯: {e}")
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    output_path = Path(output_file)
    output_path.parent.mkdir(exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(profiles, f, ensure_ascii=False, indent=2)
    
    print(f"\nç”Ÿæˆå®Œæˆï¼å…±åˆ†æ {len(profiles)} ä¸ªç­çº§ï¼Œä¿å­˜åˆ° {output_file}")
    return profiles


def analyze_with_llm(df: pd.DataFrame, class_name: str) -> Generator[str, None, Dict]:
    """
    ä½¿ç”¨å¤§æ¨¡å‹åˆ†æä½“æµ‹æ•°æ®ï¼ˆæµå¼è¾“å‡ºï¼‰

    å‚æ•°:
        df: ä½“æµ‹æ•°æ®DataFrame
        class_name: ç­çº§åç§°

    è¿”å›:
        ç”Ÿæˆå™¨ï¼Œyieldåˆ†æè¿‡ç¨‹ï¼Œæœ€åè¿”å›åˆ†æç»“æœ
    """
    from ai_model_optimized import OptimizedAIModel

    try:
        # è·å–å¹´çº§ç¼–å·
        grade_code = df['å¹´çº§ç¼–å·'].iloc[0] if len(df) > 0 else 14
        grade_query = GRADE_MAPPING.get(grade_code, "1")

        yield f"ğŸ“Š å¼€å§‹åˆ†æ {class_name} çš„ä½“æµ‹æ•°æ®...\n\n"
        yield f"âœ… æ£€æµ‹åˆ°å¹´çº§ï¼š{grade_query}å¹´çº§\n"
        yield f"âœ… å­¦ç”Ÿäººæ•°ï¼š{len(df)}äºº\n\n"

        # ç»Ÿè®¡å„é¡¹ä½“æµ‹æ•°æ®
        yield "ğŸ“ˆ æ­£åœ¨ç»Ÿè®¡å„é¡¹ä½“æµ‹æŒ‡æ ‡...\n\n"

        test_items = {
            "50ç±³è·‘": "é€Ÿåº¦",
            "ä¸€åˆ†é’Ÿä»°å§èµ·å": "åŠ›é‡",
            "åä½ä½“å‰å±ˆ": "æŸ”éŸ§",
            "ä¸€åˆ†é’Ÿè·³ç»³": "é€Ÿåº¦",
            "ç«‹å®šè·³è¿œ": "åŠ›é‡",
            "800ç±³è·‘": "è€åŠ›",
            "1000ç±³è·‘": "è€åŠ›",
            "è‚ºæ´»é‡": "æœºèƒ½",
            "èº«é«˜": "å½¢æ€",
            "ä½“é‡": "å½¢æ€"
        }

        stats_text = ""
        for item, dimension in test_items.items():
            grade_col = f"{item}ç­‰çº§"
            if grade_col in df.columns:
                grade_counts = df[grade_col].value_counts()
                total = len(df[df[grade_col].notna()])
                if total > 0:
                    stats_text += f"- {item}ï¼ˆ{dimension}ï¼‰ï¼šä¼˜ç§€{grade_counts.get('ä¼˜ç§€', 0)}äººï¼Œè‰¯å¥½{grade_counts.get('è‰¯å¥½', 0)}äººï¼ŒåŠæ ¼{grade_counts.get('åŠæ ¼', 0)}äººï¼Œä¸åŠæ ¼{grade_counts.get('ä¸åŠæ ¼', 0)}äºº\n"

        yield stats_text + "\n"

        # è°ƒç”¨å¤§æ¨¡å‹åˆ†æ
        yield "ğŸ¤– æ­£åœ¨ä½¿ç”¨AIåˆ†æè–„å¼±é¡¹...\n\n"

        model = OptimizedAIModel()

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä½“è‚²æ•™å¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹ç­çº§çš„ä½“æµ‹æ•°æ®ï¼Œè¯†åˆ«è–„å¼±é¡¹ã€‚

ç­çº§ï¼š{class_name}
å¹´çº§ï¼š{grade_query}å¹´çº§
å­¦ç”Ÿäººæ•°ï¼š{len(df)}äºº

å„é¡¹ä½“æµ‹æ•°æ®ç»Ÿè®¡ï¼š
{stats_text}

**é‡è¦è§„åˆ™ï¼š**
1. è–„å¼±é¡¹åªèƒ½ä»ä»¥ä¸‹6ä¸ªç»´åº¦ä¸­é€‰æ‹©ï¼šå½¢æ€ã€è€åŠ›ã€åŠ›é‡ã€æŸ”éŸ§ã€é€Ÿåº¦ã€æœºèƒ½
2. è¯·é€‰æ‹©æœ€è–„å¼±çš„1-2ä¸ªç»´åº¦
3. å¯¹æ¯ä¸ªè–„å¼±ç»´åº¦ï¼Œç»™å‡ºè¯¦ç»†çš„åˆ†æè¯´æ˜

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
```json
{{
    "weaknesses": ["ç»´åº¦1", "ç»´åº¦2"],
    "weakness_details": {{
        "ç»´åº¦1": "è¯¦ç»†åˆ†æè¯´æ˜...",
        "ç»´åº¦2": "è¯¦ç»†åˆ†æè¯´æ˜..."
    }}
}}
```"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä½“è‚²æ•™å¸ˆï¼Œæ“…é•¿åˆ†æå­¦ç”Ÿä½“æµ‹æ•°æ®ã€‚"},
            {"role": "user", "content": prompt}
        ]

        response = model.client.chat.completions.create(
            model=model.model,
            messages=messages,
            max_tokens=1000,
            temperature=0.3
        )

        response_text = response.choices[0].message.content.strip()
        yield f"AIåˆ†æç»“æœï¼š\n{response_text}\n\n"

        # è§£æJSONç»“æœ
        import re
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
        if json_match:
            analysis_result = json.loads(json_match.group(1))
            weaknesses = analysis_result.get('weaknesses', [])
            weakness_details = analysis_result.get('weakness_details', {})
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•åˆ†æ
            yield "âš ï¸ AIè¿”å›æ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•åˆ†æ...\n\n"
            weaknesses, weakness_details, _ = analyze_class_weakness(df, class_name)

        # ç¡®ä¿è–„å¼±é¡¹åœ¨å…è®¸çš„èŒƒå›´å†…
        weaknesses = [w for w in weaknesses if w in ALLOWED_WEAKNESSES][:2]

        yield f"âœ… è¯†åˆ«åˆ°è–„å¼±é¡¹ï¼š{', '.join(weaknesses)}\n\n"

        # æ„å»ºæè¿°
        description = f"{class_name}ä½“è´¨ç›‘æµ‹æ ¸å¿ƒè–„å¼±ç»´åº¦ï¼š" + "ã€".join(weaknesses) if weaknesses else f"{class_name}ä½“è´¨ç›‘æµ‹æ•°æ®"

        profile = {
            "grades_query": grade_query,
            "trained_weaknesses": "ã€".join(weaknesses) if weaknesses else "",
            "count_query": "",
            "semantic_query": "",
            "description": description,
            "weakness_details": weakness_details
        }

        yield "ğŸ’¾ æ­£åœ¨ä¿å­˜é…ç½®...\n"

        # ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ¥æ ‡è¯†è¿™æ˜¯æœ€ç»ˆç»“æœ
        yield ("__PROFILE__", profile)

    except Exception as e:
        yield f"âŒ åˆ†æå¤±è´¥ï¼š{str(e)}\n"
        raise e


def analyze_uploaded_file(file_content: bytes, class_name: str, output_file: str = "prompts/class_profiles.json") -> Dict:
    """
    åˆ†æä¸Šä¼ çš„ä½“æµ‹æ•°æ®æ–‡ä»¶

    å‚æ•°:
        file_content: æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
        class_name: ç­çº§åç§°
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„

    è¿”å›:
        åˆ†æç»“æœå­—å…¸
    """
    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(io.BytesIO(file_content))

        # è·å–å¹´çº§ç¼–å·
        grade_code = df['å¹´çº§ç¼–å·'].iloc[0] if len(df) > 0 else 14
        grade_query = GRADE_MAPPING.get(grade_code, "1")

        # åˆ†æè–„å¼±é¡¹
        weaknesses, weakness_details, weakness_test_items = analyze_class_weakness(df, class_name)

        # æ„å»ºç­çº§é…ç½®
        weakness_desc_items = []
        for weakness in weaknesses:
            test_item = weakness_test_items.get(weakness, "")
            if test_item:
                weakness_desc_items.append(f"{weakness}ï¼ˆ{test_item}ï¼‰")
            else:
                weakness_desc_items.append(weakness)

        description = f"{class_name}ä½“è´¨ç›‘æµ‹æ ¸å¿ƒè–„å¼±ç»´åº¦ï¼š" + "ã€".join(weakness_desc_items) if weakness_desc_items else f"{class_name}ä½“è´¨ç›‘æµ‹æ•°æ®"

        profile = {
            "grades_query": grade_query,
            "trained_weaknesses": "ã€".join(weaknesses) if weaknesses else "",
            "count_query": "",
            "semantic_query": "",
            "description": description,
            "weakness_details": weakness_details
        }

        # æ›´æ–°JSONæ–‡ä»¶
        update_class_profile(class_name, profile, output_file)

        return {
            "success": True,
            "class_name": class_name,
            "profile": profile
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


def update_class_profile(class_name: str, profile: Dict, output_file: str = "prompts/class_profiles.json"):
    """
    æ›´æ–°class_profiles.jsonæ–‡ä»¶

    å‚æ•°:
        class_name: ç­çº§åç§°
        profile: ç­çº§é…ç½®
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
    """
    output_path = Path(output_file)
    output_path.parent.mkdir(exist_ok=True)

    # è¯»å–ç°æœ‰é…ç½®
    if output_path.exists():
        try:
            with open(output_path, "r", encoding="utf-8") as f:
                content = f.read().strip()
                if content:
                    profiles = json.loads(content)
                else:
                    profiles = {}
        except (json.JSONDecodeError, ValueError):
            # å¦‚æœJSONæ–‡ä»¶æŸåæˆ–ä¸ºç©ºï¼Œåˆå§‹åŒ–ä¸ºç©ºå­—å…¸
            profiles = {}
    else:
        profiles = {}

    # æ›´æ–°é…ç½®
    profiles[class_name] = profile

    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(profiles, f, ensure_ascii=False, indent=2)


def delete_class_profile(class_name: str, output_file: str = "prompts/class_profiles.json") -> bool:
    """
    åˆ é™¤ç­çº§é…ç½®

    å‚æ•°:
        class_name: ç­çº§åç§°
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„

    è¿”å›:
        æ˜¯å¦åˆ é™¤æˆåŠŸ
    """
    output_path = Path(output_file)

    if not output_path.exists():
        return False

    try:
        with open(output_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if content:
                profiles = json.loads(content)
            else:
                profiles = {}
    except (json.JSONDecodeError, ValueError):
        # å¦‚æœJSONæ–‡ä»¶æŸåæˆ–ä¸ºç©ºï¼Œè¿”å›False
        return False

    if class_name in profiles:
        del profiles[class_name]

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(profiles, f, ensure_ascii=False, indent=2)

        return True

    return False


def get_all_class_profiles(output_file: str = "prompts/class_profiles.json") -> Dict:
    """
    è·å–æ‰€æœ‰ç­çº§é…ç½®

    å‚æ•°:
        output_file: JSONæ–‡ä»¶è·¯å¾„

    è¿”å›:
        æ‰€æœ‰ç­çº§é…ç½®å­—å…¸
    """
    output_path = Path(output_file)

    if not output_path.exists():
        return {}

    try:
        with open(output_path, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if content:
                return json.loads(content)
            else:
                return {}
    except (json.JSONDecodeError, ValueError):
        # å¦‚æœJSONæ–‡ä»¶æŸåæˆ–ä¸ºç©ºï¼Œè¿”å›ç©ºå­—å…¸
        return {}


if __name__ == "__main__":
    # æµ‹è¯•ï¼šåˆ†æclass_dataæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ç­çº§
    generate_class_profiles(max_classes=None)

